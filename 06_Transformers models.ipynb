{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47b5a130-8c0f-42cf-b055-5128a8c1ff55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Model Selection Configuration\n",
    "Set to True for models you want to train, False for models to skip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66862e2e-c8b0-4d3a-94d4-c9b4f6e6f76c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\nRequirement already satisfied: spark-nlp in /local_disk0/.ephemeral_nfs/envs/pythonEnv-94fad2c6-feb0-4207-8727-c818ea380fb4/lib/python3.9/site-packages (6.0.2)\nPython interpreter will be restarted.\n"
     ]
    }
   ],
   "source": [
    "%pip install spark-nlp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5d8a533-2668-4638-8ae9-a6fdcd057806",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Model selection configuration - Set to True to run, False to skip\n",
    "RUN_MODELS = {\n",
    "    'transformer_classifier': True,\n",
    "    'bert_features': True, \n",
    "    'transformer_mlp': True,\n",
    "    'distilbert_classifier': True,\n",
    "    'roberta_features': False\n",
    "}\n",
    "\n",
    "# Random search configuration\n",
    "RANDOM_SEARCH_CONFIG = {\n",
    "    'num_folds': 3,\n",
    "    'parallelism': 1\n",
    "}\n",
    "# File paths configuration\n",
    "DATA_PATHS = {\n",
    "    'train': \"dbfs:/FileStore/tables/train_df.csv\",\n",
    "    'val': \"dbfs:/FileStore/tables/val_df.csv\",\n",
    "    'test': \"dbfs:/FileStore/tables/test_df.csv\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ef5edb8-b052-486c-a5e2-f258b18a1d5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "226a9a4e-9811-4864-ae39-c46d3c8446ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark NLP available - using transformer models\n"
     ]
    }
   ],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.lines import Line2D\n",
    "from math import ceil\n",
    "import time\n",
    "import os\n",
    "\n",
    "# PySpark imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, row_number, when, lit, count, lag, expr, concat_ws, collect_list\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType, ArrayType, FloatType, StringType\n",
    "\n",
    "# ML imports for classification\n",
    "from pyspark.ml.classification import LogisticRegression, MultilayerPerceptronClassifier\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, StringIndexer, IndexToString\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Scikit-learn imports for metrics (only for visualization)\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, precision_recall_curve\n",
    "\n",
    "# Spark NLP imports for transformer models\n",
    "try:\n",
    "    import sparknlp\n",
    "    from sparknlp.base import *\n",
    "    from sparknlp.annotator import *\n",
    "    from sparknlp.pretrained import PretrainedPipeline\n",
    "    SPARK_NLP_AVAILABLE = True\n",
    "    print(\"Spark NLP available - using transformer models\")\n",
    "except ImportError:\n",
    "    SPARK_NLP_AVAILABLE = False\n",
    "    print(\"Spark NLP not available - using alternative transformer approaches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a970d4f4-ce0f-418d-87a3-2f9e5cd9fca2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. Loading and Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9db5944-f76c-484f-a2f3-26fe64d7134c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(file_paths):\n",
    "    \"\"\"Load and preprocess data from specified file paths.\"\"\"\n",
    "    # Load data\n",
    "    train_data = spark.read.csv(file_paths['train'], header=True, inferSchema=True)\n",
    "    val_data = spark.read.csv(file_paths['val'], header=True, inferSchema=True)\n",
    "    test_data = spark.read.csv(file_paths['test'], header=True, inferSchema=True)\n",
    "    \n",
    "    # Convert numerical features to text for transformer processing\n",
    "    feature_cols = [col for col in train_data.columns if col not in ['label', 'time', 'file']]\n",
    "    \n",
    "    # Create text representation of features for transformer models\n",
    "    # Combine all features into a single text column\n",
    "    def create_text_features(df):\n",
    "        # Convert all feature columns to string and concatenate\n",
    "        text_expr = concat_ws(\" \", *[col(c).cast(\"string\") for c in feature_cols])\n",
    "        df = df.withColumn(\"text_features\", text_expr)\n",
    "        return df\n",
    "    \n",
    "    train_data = create_text_features(train_data)\n",
    "    val_data = create_text_features(val_data)\n",
    "    test_data = create_text_features(test_data)\n",
    "    \n",
    "    # Also keep original features for hybrid approaches\n",
    "    assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "    train_data = assembler.transform(train_data)\n",
    "    val_data = assembler.transform(val_data)\n",
    "    test_data = assembler.transform(test_data)\n",
    "    \n",
    "    print(f\"Data loaded and preprocessed:\")\n",
    "    print(f\" - Training samples: {train_data.count()}\")\n",
    "    print(f\" - Validation samples: {val_data.count()}\")\n",
    "    print(f\" - Test samples: {test_data.count()}\")\n",
    "    \n",
    "    return train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b69cea8f-80f6-491b-a96a-2a7995494c97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54a4b739-733b-4dc9-9666-b4c64ca93e32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Helper function to convert PySpark predictions to numpy arrays for plotting\n",
    "def get_prediction_labels(predictions_df):\n",
    "    \"\"\"Extract prediction and label columns from PySpark DataFrame.\"\"\"\n",
    "    pred_labels = predictions_df.select(\"prediction\", \"label\").toPandas()\n",
    "    y_pred = pred_labels[\"prediction\"].values\n",
    "    y_true = pred_labels[\"label\"].values\n",
    "    return y_pred, y_true\n",
    "\n",
    "# Helper function to get prediction probabilities\n",
    "def get_prediction_probabilities(predictions_df):\n",
    "    \"\"\"Extract probability column from PySpark DataFrame.\"\"\"\n",
    "    prob_df = predictions_df.select(\"probability\").toPandas()\n",
    "    return np.array([x.toArray() for x in prob_df[\"probability\"]])\n",
    "\n",
    "# Helper function to plot confusion matrix\n",
    "def plot_confusion_matrix(y_true, y_pred, title=\"Confusion Matrix\"):\n",
    "    \"\"\"Plot confusion matrix using seaborn.\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "\n",
    "# Helper function to print classification report\n",
    "def print_classification_report(y_true, y_pred):\n",
    "    \"\"\"Print classification report with precision, recall, and F1 scores.\"\"\"\n",
    "    report = classification_report(y_true, y_pred)\n",
    "    print(\"Classification Report:\")\n",
    "    print(report)\n",
    "\n",
    "def evaluate_model(model_name, val_predictions, num_classes, has_probability=True):\n",
    "    \"\"\"Perform comprehensive evaluation of a model.\"\"\"\n",
    "    print(f\"\\n--- {model_name} Evaluation ---\")\n",
    "    \n",
    "    # Extract predictions and true labels\n",
    "    y_pred, y_true = get_prediction_labels(val_predictions)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    plot_confusion_matrix(y_true, y_pred, f\"{model_name} Confusion Matrix\")\n",
    "    \n",
    "    # Print classification report\n",
    "    print(f\"\\n{model_name} Classification Report:\")\n",
    "    print_classification_report(y_true, y_pred)\n",
    "    \n",
    "    # If model has probability outputs, plot ROC and PR curves\n",
    "    y_pred_proba = None\n",
    "    if has_probability:\n",
    "        try:\n",
    "            y_pred_proba = get_prediction_probabilities(val_predictions)\n",
    "            print(f\"{model_name} has probability outputs available\")\n",
    "        except:\n",
    "            print(f\"{model_name} probability outputs not available\")\n",
    "            has_probability = False\n",
    "    \n",
    "    return y_pred, y_true, y_pred_proba\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea0c9de5-b243-494f-a853-f8d37bf0f381",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def perform_random_search(model, param_grid, train_data, val_data, num_folds=3, parallelism=2):\n",
    "    \"\"\"Perform random search for hyperparameter tuning of a model.\"\"\"\n",
    "    # Initialize the evaluator for F1 score\n",
    "    evaluator = MulticlassClassificationEvaluator(\n",
    "        labelCol='label',\n",
    "        predictionCol='prediction',\n",
    "        metricName='f1'\n",
    "    )\n",
    "    \n",
    "    # Initialize CrossValidator for hyperparameter tuning\n",
    "    cv = CrossValidator(\n",
    "        estimator=model,\n",
    "        estimatorParamMaps=param_grid,\n",
    "        evaluator=evaluator,\n",
    "        numFolds=num_folds,\n",
    "        parallelism=parallelism\n",
    "    )\n",
    "    \n",
    "    # Start timing\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Fit the cross-validator to the training data\n",
    "    print(\"Training model with random search...\")\n",
    "    cv_model = cv.fit(train_data)\n",
    "    \n",
    "    # End timing\n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"Training completed in {training_time:.2f} seconds\")\n",
    "    \n",
    "    # Extract the best model\n",
    "    best_model = cv_model.bestModel\n",
    "    \n",
    "    # Get best parameters\n",
    "    best_params = {}\n",
    "    for param in best_model.extractParamMap():\n",
    "        param_name = param.name\n",
    "        param_value = best_model.getOrDefault(param)\n",
    "        best_params[param_name] = param_value\n",
    "    \n",
    "    # Make predictions with the best model\n",
    "    train_predictions = best_model.transform(train_data)\n",
    "    val_predictions = best_model.transform(val_data)\n",
    "    \n",
    "    # Calculate F1 scores\n",
    "    train_f1 = evaluator.evaluate(train_predictions)\n",
    "    val_f1 = evaluator.evaluate(val_predictions)\n",
    "    \n",
    "    return best_model, best_params, train_predictions, val_predictions, train_f1, val_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f00facbb-7eb4-4155-8e3d-199ce5c10d4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5 - Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d605017-d242-411c-8c91-356d176da217",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded and preprocessed:\n - Training samples: 3142910\n - Validation samples: 681381\n - Test samples: 681395\n\nSample of training data:\n+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+--------+-----+--------------------+--------------------+\n|              FP1-F7|               F7-T7|               T7-P7|               P7-O1|              FP1-F3|               F3-C3|               C3-P3|               P3-O1|              FP2-F4|               F4-C4|               C4-P4|               P4-O2|              FP2-F8|               F8-T8|             T8-P8-0|               P8-O2|               FZ-CZ|               CZ-PZ|               P7-T7|              T7-FT9|            FT9-FT10|             FT10-T8|             T8-P8-1|      time|    file|label|       text_features|            features|\n+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+--------+-----+--------------------+--------------------+\n|  -1.611721611721E-4|2.637362637362637E-5|-1.95360195360196...|7.873015873015874E-5|-4.16117216117216...|7.833943833943834E-5|  -3.936507936507E-4|   3.006593406593E-4|   1.799267399267E-4|8.029304029304029E-5|-6.70085470085470...|   1.611721611721E-4|   1.662515262515E-4|3.731379731379732E-5|5.958485958485958E-5|9.123321123321125E-5|1.113553113553113...|7.755799755799755E-5|5.860805860805853E-7|5.411477411477411E-5|2.676434676434676...|5.528693528693529E-5|5.958485958485958E-5|       0.0|chb08_02|    0|-1.611721611721E-...|[-1.611721611721E...|\n|1.953601953601946...|1.953601953601946...|1.953601953601946...|1.953601953601946...|1.953601953601946...|1.953601953601946...|1.953601953601946...|1.953601953601946...|1.953601953601946...|1.953601953601946...|1.953601953601946...|1.953601953601946...|1.953601953601946...|1.953601953601946...|1.953601953601946...|1.953601953601946...|1.953601953601946...|1.953601953601946...|1.953601953601946...|1.953601953601946...|1.953601953601946...|1.953601953601946...|1.953601953601946...|0.00390625|chb08_02|    0|1.953601953601946...|[1.95360195360194...|\n|-9.76800976800977...|1.953601953601946...|-1.36752136752136...|3.321123321123320...|-9.76800976800977...|1.953601953601946...|-1.95360195360196...|2.148962148962148...|-9.76800976800977...|-1.75824175824175...|-1.95360195360196...|3.711843711843711E-6|-1.36752136752136...|-1.95360195360196...|5.860805860805853E-7|1.758241758241757...|1.953601953601946...|-5.86080586080586...|1.758241758241757...|-2.53968253968254...|2.930402930402929E-6|-1.95360195360196...|5.860805860805853E-7|0.01171875|chb08_02|    0|-9.76800976800977...|[-9.7680097680097...|\n+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+--------+-----+--------------------+--------------------+\nonly showing top 3 rows\n\n\nNumber of classes: 3\n"
     ]
    }
   ],
   "source": [
    "class TransformerFeatureExtractor:\n",
    "    \"\"\"Custom transformer feature extractor using Spark NLP\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"bert_base_uncased\"):\n",
    "        self.model_name = model_name\n",
    "        \n",
    "    def create_pipeline(self):\n",
    "        \"\"\"Create Spark NLP pipeline for feature extraction\"\"\"\n",
    "        if not SPARK_NLP_AVAILABLE:\n",
    "            raise ImportError(\"Spark NLP not available\")\n",
    "        \n",
    "        # Document assembler\n",
    "        document_assembler = DocumentAssembler() \\\n",
    "            .setInputCol(\"text_features\") \\\n",
    "            .setOutputCol(\"document\")\n",
    "        \n",
    "        # Tokenizer\n",
    "        tokenizer = Tokenizer() \\\n",
    "            .setInputCols([\"document\"]) \\\n",
    "            .setOutputCol(\"token\")\n",
    "        \n",
    "        # BERT embeddings\n",
    "        bert_embeddings = BertEmbeddings.pretrained(\"bert_base_uncased\", \"en\") \\\n",
    "            .setInputCols([\"document\", \"token\"]) \\\n",
    "            .setOutputCol(\"bert_embeddings\") \\\n",
    "            .setCaseSensitive(False) \\\n",
    "            .setMaxSentenceLength(512)\n",
    "        \n",
    "        # Sentence embeddings (pooling)\n",
    "        sentence_embeddings = SentenceEmbeddings() \\\n",
    "            .setInputCols([\"document\", \"bert_embeddings\"]) \\\n",
    "            .setOutputCol(\"sentence_embeddings\") \\\n",
    "            .setPoolingStrategy(\"AVERAGE\")\n",
    "        \n",
    "        return Pipeline(stages=[\n",
    "            document_assembler,\n",
    "            tokenizer, \n",
    "            bert_embeddings,\n",
    "            sentence_embeddings\n",
    "        ])\n",
    "\n",
    "def create_transformer_mlp_model(input_dim, hidden_layers, num_classes):\n",
    "    \"\"\"Create deep MLP that works with transformer features\"\"\"\n",
    "    layers = [input_dim] + hidden_layers + [num_classes]\n",
    "    \n",
    "    return MultilayerPerceptronClassifier(\n",
    "        labelCol=\"label\",\n",
    "        featuresCol=\"transformer_features\",\n",
    "        layers=layers,\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "# Load and preprocess the data\n",
    "train_data, val_data, test_data = load_and_preprocess_data(DATA_PATHS)\n",
    "\n",
    "# Display a few samples\n",
    "print(\"\\nSample of training data:\")\n",
    "train_data.show(3)\n",
    "\n",
    "# Count classes\n",
    "num_classes = train_data.select(\"label\").distinct().count()\n",
    "print(f\"\\nNumber of classes: {num_classes}\")\n",
    "\n",
    "# Store model results\n",
    "model_names = []\n",
    "val_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43892373-637e-44d0-aefb-26d5800b0848",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n==== Transformer Classifier Model ====\nError with Transformer Classifier: 'JavaPackage' object is not callable\nSkipping Transformer Classifier\n"
     ]
    }
   ],
   "source": [
    "# ==================== TRANSFORMER CLASSIFIER ====================\n",
    "if RUN_MODELS['transformer_classifier'] and SPARK_NLP_AVAILABLE:\n",
    "    print(\"\\n==== Transformer Classifier Model ====\")\n",
    "    \n",
    "    try:\n",
    "        # Create transformer feature extractor\n",
    "        transformer_extractor = TransformerFeatureExtractor()\n",
    "        feature_pipeline = transformer_extractor.create_pipeline()\n",
    "        \n",
    "        # Extract features\n",
    "        print(\"Extracting transformer features...\")\n",
    "        feature_model = feature_pipeline.fit(train_data)\n",
    "        train_transformed = feature_model.transform(train_data)\n",
    "        val_transformed = feature_model.transform(val_data)\n",
    "        \n",
    "        # Convert embeddings to vector features\n",
    "        def extract_embeddings_udf(embeddings):\n",
    "            if embeddings and len(embeddings) > 0:\n",
    "                return embeddings[0].embeddings.tolist()\n",
    "            else:\n",
    "                return [0.0] * 768  # BERT base dimension\n",
    "        \n",
    "        extract_udf = udf(extract_embeddings_udf, ArrayType(FloatType()))\n",
    "        \n",
    "        # Apply UDF to extract embeddings\n",
    "        train_with_features = train_transformed.withColumn(\"embedding_array\", extract_udf(\"sentence_embeddings\"))\n",
    "        val_with_features = val_transformed.withColumn(\"embedding_array\", extract_udf(\"sentence_embeddings\"))\n",
    "        \n",
    "        # Convert to vector format\n",
    "        from pyspark.ml.functions import array_to_vector\n",
    "        train_final = train_with_features.withColumn(\"transformer_features\", array_to_vector(\"embedding_array\"))\n",
    "        val_final = val_with_features.withColumn(\"transformer_features\", array_to_vector(\"embedding_array\"))\n",
    "        \n",
    "        # Create classifier on top of transformer features\n",
    "        transformer_classifier = LogisticRegression(\n",
    "            labelCol=\"label\",\n",
    "            featuresCol=\"transformer_features\"\n",
    "        )\n",
    "        \n",
    "        # Parameter grid for transformer classifier\n",
    "        tc_param_grid = ParamGridBuilder() \\\n",
    "            .addGrid(transformer_classifier.regParam, [0.01, 0.1]) \\\n",
    "            .addGrid(transformer_classifier.elasticNetParam, [0.0, 0.5]) \\\n",
    "            .build()\n",
    "        \n",
    "        # Perform random search\n",
    "        tc_best_model, tc_best_params, tc_train_preds, tc_val_preds, tc_train_f1, tc_val_f1 = perform_random_search(\n",
    "            transformer_classifier, tc_param_grid, train_final, val_final,\n",
    "            num_folds=RANDOM_SEARCH_CONFIG['num_folds'],\n",
    "            parallelism=RANDOM_SEARCH_CONFIG['parallelism']\n",
    "        )\n",
    "        \n",
    "        # Print best parameters and performance\n",
    "        print(\"\\nBest Transformer Classifier Parameters:\")\n",
    "        for param, value in tc_best_params.items():\n",
    "            print(f\" {param}: {value}\")\n",
    "        \n",
    "        print(f\"\\nTransformer Classifier - Training F1 Score: {tc_train_f1:.4f}\")\n",
    "        print(f\"Transformer Classifier - Validation F1 Score: {tc_val_f1:.4f}\")\n",
    "        \n",
    "        # Run comprehensive evaluation\n",
    "        tc_y_pred, tc_y_true, tc_y_pred_proba = evaluate_model(\"Transformer Classifier\", tc_val_preds, num_classes)\n",
    "        \n",
    "        model_names.append(\"Transformer Classifier\")\n",
    "        val_scores.append(tc_val_f1)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error with Transformer Classifier: {e}\")\n",
    "        print(\"Skipping Transformer Classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e3024e1-752c-4425-b727-954b51507134",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n==== BERT Features + MLP Model ====\nTransformer features not available, skipping BERT + MLP\n"
     ]
    }
   ],
   "source": [
    "# ==================== BERT FEATURES + MLP ====================\n",
    "if RUN_MODELS['bert_features']:\n",
    "    print(\"\\n==== BERT Features + MLP Model ====\")\n",
    "    \n",
    "    if SPARK_NLP_AVAILABLE:\n",
    "        try:\n",
    "            # Use same transformer features as above but with MLP\n",
    "            if 'train_final' in locals():\n",
    "                # Create MLP with transformer features\n",
    "                bert_mlp = create_transformer_mlp_model(\n",
    "                    input_dim=768,  # BERT base dimension\n",
    "                    hidden_layers=[256, 128, 64],\n",
    "                    num_classes=num_classes\n",
    "                )\n",
    "                \n",
    "                # Parameter grid for BERT MLP\n",
    "                bert_param_grid = ParamGridBuilder() \\\n",
    "                    .addGrid(bert_mlp.blockSize, [32, 64]) \\\n",
    "                    .addGrid(bert_mlp.maxIter, [50, 100]) \\\n",
    "                    .addGrid(bert_mlp.stepSize, [0.01, 0.03]) \\\n",
    "                    .build()\n",
    "                \n",
    "                # Perform random search\n",
    "                bert_best_model, bert_best_params, bert_train_preds, bert_val_preds, bert_train_f1, bert_val_f1 = perform_random_search(\n",
    "                    bert_mlp, bert_param_grid, train_final, val_final,\n",
    "                    num_folds=RANDOM_SEARCH_CONFIG['num_folds'],\n",
    "                    parallelism=RANDOM_SEARCH_CONFIG['parallelism']\n",
    "                )\n",
    "                \n",
    "                print(\"\\nBest BERT + MLP Parameters:\")\n",
    "                for param, value in bert_best_params.items():\n",
    "                    print(f\" {param}: {value}\")\n",
    "                \n",
    "                print(f\"\\nBERT + MLP - Training F1 Score: {bert_train_f1:.4f}\")\n",
    "                print(f\"BERT + MLP - Validation F1 Score: {bert_val_f1:.4f}\")\n",
    "                \n",
    "                # Run comprehensive evaluation\n",
    "                bert_y_pred, bert_y_true, _ = evaluate_model(\"BERT + MLP\", bert_val_preds, num_classes, has_probability=False)\n",
    "                \n",
    "                model_names.append(\"BERT + MLP\")\n",
    "                val_scores.append(bert_val_f1)\n",
    "            else:\n",
    "                print(\"Transformer features not available, skipping BERT + MLP\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error with BERT + MLP: {e}\")\n",
    "    else:\n",
    "        print(\"Spark NLP not available, skipping BERT + MLP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1e289a2-60ed-4794-8bfb-1eeea65d86c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n==== Transformer-Inspired MLP Model ====\nTraining model with random search...\n"
     ]
    }
   ],
   "source": [
    "# ==================== TRANSFORMER-INSPIRED MLP ====================\n",
    "if RUN_MODELS['transformer_mlp']:\n",
    "    print(\"\\n==== Transformer-Inspired MLP Model ====\")\n",
    "    \n",
    "    # Create transformer-inspired features using original numerical features\n",
    "    # Simulate attention mechanisms through feature interactions\n",
    "    def create_attention_features(df, feature_cols):\n",
    "        # Create pairwise feature interactions (attention-like)\n",
    "        attention_cols = []\n",
    "        \n",
    "        # Select top features to avoid combinatorial explosion\n",
    "        top_features = feature_cols[:10]\n",
    "        \n",
    "        for i, feat1 in enumerate(top_features):\n",
    "            for j, feat2 in enumerate(top_features[i+1:], i+1):\n",
    "                interaction_col = f\"attn_{i}_{j}\"\n",
    "                df = df.withColumn(interaction_col, col(feat1) * col(feat2))\n",
    "                attention_cols.append(interaction_col)\n",
    "        \n",
    "        return df, attention_cols\n",
    "    \n",
    "    # Get original feature columns\n",
    "    feature_cols = [col for col in train_data.columns if col not in ['label', 'time', 'file', 'text_features', 'features']]\n",
    "    \n",
    "    # Create attention-like features\n",
    "    train_attention, attention_cols = create_attention_features(train_data, feature_cols)\n",
    "    val_attention, _ = create_attention_features(val_data, feature_cols)\n",
    "    \n",
    "    # Combine original and attention features\n",
    "    all_features = feature_cols + attention_cols\n",
    "    \n",
    "    # Assemble features\n",
    "    transformer_assembler = VectorAssembler(inputCols=all_features, outputCol=\"transformer_inspired_features\")\n",
    "    train_attention_final = transformer_assembler.transform(train_attention)\n",
    "    val_attention_final = transformer_assembler.transform(val_attention)\n",
    "    \n",
    "    # Create deep MLP (transformer-inspired architecture)\n",
    "    transformer_inspired_mlp = MultilayerPerceptronClassifier(\n",
    "        labelCol=\"label\",\n",
    "        featuresCol=\"transformer_inspired_features\",\n",
    "        layers=[len(all_features), 512, 256, 128, 64, num_classes],  # Deep architecture\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    # Parameter grid for transformer-inspired MLP\n",
    "    tmlp_param_grid = ParamGridBuilder() \\\n",
    "        .addGrid(transformer_inspired_mlp.blockSize, [32, 64]) \\\n",
    "        .addGrid(transformer_inspired_mlp.maxIter, [100, 200]) \\\n",
    "        .addGrid(transformer_inspired_mlp.stepSize, [0.001, 0.01]) \\\n",
    "        .build()\n",
    "    \n",
    "    # Perform random search\n",
    "    tmlp_best_model, tmlp_best_params, tmlp_train_preds, tmlp_val_preds, tmlp_train_f1, tmlp_val_f1 = perform_random_search(\n",
    "        transformer_inspired_mlp, tmlp_param_grid, train_attention_final, val_attention_final,\n",
    "        num_folds=RANDOM_SEARCH_CONFIG['num_folds'],\n",
    "        parallelism=RANDOM_SEARCH_CONFIG['parallelism']\n",
    "    )\n",
    "    \n",
    "    print(\"\\nBest Transformer-Inspired MLP Parameters:\")\n",
    "    for param, value in tmlp_best_params.items():\n",
    "        print(f\" {param}: {value}\")\n",
    "    \n",
    "    print(f\"\\nTransformer-Inspired MLP - Training F1 Score: {tmlp_train_f1:.4f}\")\n",
    "    print(f\"Transformer-Inspired MLP - Validation F1 Score: {tmlp_val_f1:.4f}\")\n",
    "    \n",
    "    # Run comprehensive evaluation\n",
    "    tmlp_y_pred, tmlp_y_true, _ = evaluate_model(\"Transformer-Inspired MLP\", tmlp_val_preds, num_classes, has_probability=False)\n",
    "    \n",
    "    model_names.append(\"Transformer-Inspired MLP\")\n",
    "    val_scores.append(tmlp_val_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8005236d-97c5-4307-8c5a-15d7081e58ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==================== DISTILBERT CLASSIFIER ====================\n",
    "if RUN_MODELS['distilbert_classifier'] and SPARK_NLP_AVAILABLE:\n",
    "    print(\"\\n==== DistilBERT Classifier Model ====\")\n",
    "    \n",
    "    try:\n",
    "        # Create DistilBERT pipeline\n",
    "        document_assembler = DocumentAssembler() \\\n",
    "            .setInputCol(\"text_features\") \\\n",
    "            .setOutputCol(\"document\")\n",
    "        \n",
    "        tokenizer = Tokenizer() \\\n",
    "            .setInputCols([\"document\"]) \\\n",
    "            .setOutputCol(\"token\")\n",
    "        \n",
    "        # Use DistilBERT embeddings\n",
    "        distilbert_embeddings = DistilBertEmbeddings.pretrained(\"distilbert_base_uncased\", \"en\") \\\n",
    "            .setInputCols([\"document\", \"token\"]) \\\n",
    "            .setOutputCol(\"distilbert_embeddings\") \\\n",
    "            .setCaseSensitive(False)\n",
    "        \n",
    "        sentence_embeddings = SentenceEmbeddings() \\\n",
    "            .setInputCols([\"document\", \"distilbert_embeddings\"]) \\\n",
    "            .setOutputCol(\"sentence_embeddings\") \\\n",
    "            .setPoolingStrategy(\"AVERAGE\")\n",
    "        \n",
    "        distilbert_pipeline = Pipeline(stages=[\n",
    "            document_assembler,\n",
    "            tokenizer,\n",
    "            distilbert_embeddings,\n",
    "            sentence_embeddings\n",
    "        ])\n",
    "        \n",
    "        # Extract DistilBERT features\n",
    "        print(\"Extracting DistilBERT features...\")\n",
    "        distilbert_model = distilbert_pipeline.fit(train_data)\n",
    "        train_distilbert = distilbert_model.transform(train_data)\n",
    "        val_distilbert = distilbert_model.transform(val_data)\n",
    "        \n",
    "        # Process embeddings (similar to BERT processing)\n",
    "        train_distilbert_features = train_distilbert.withColumn(\"embedding_array\", extract_udf(\"sentence_embeddings\"))\n",
    "        val_distilbert_features = val_distilbert.withColumn(\"embedding_array\", extract_udf(\"sentence_embeddings\"))\n",
    "        \n",
    "        train_distilbert_final = train_distilbert_features.withColumn(\"distilbert_features\", array_to_vector(\"embedding_array\"))\n",
    "        val_distilbert_final = val_distilbert_features.withColumn(\"distilbert_features\", array_to_vector(\"embedding_array\"))\n",
    "        \n",
    "        # Create classifier\n",
    "        distilbert_classifier = LogisticRegression(\n",
    "            labelCol=\"label\",\n",
    "            featuresCol=\"distilbert_features\"\n",
    "        )\n",
    "        \n",
    "        # Parameter grid\n",
    "        db_param_grid = ParamGridBuilder() \\\n",
    "            .addGrid(distilbert_classifier.regParam, [0.01, 0.1]) \\\n",
    "            .addGrid(distilbert_classifier.elasticNetParam, [0.0, 0.5]) \\\n",
    "            .build()\n",
    "        \n",
    "        # Perform random search\n",
    "        db_best_model, db_best_params, db_train_preds, db_val_preds, db_train_f1, db_val_f1 = perform_random_search(\n",
    "            distilbert_classifier, db_param_grid, train_distilbert_final, val_distilbert_final,\n",
    "            num_folds=RANDOM_SEARCH_CONFIG['num_folds'],\n",
    "            parallelism=RANDOM_SEARCH_CONFIG['parallelism']\n",
    "        )\n",
    "        \n",
    "        print(\"\\nBest DistilBERT Classifier Parameters:\")\n",
    "        for param, value in db_best_params.items():\n",
    "            print(f\" {param}: {value}\")\n",
    "        \n",
    "        print(f\"\\nDistilBERT Classifier - Training F1 Score: {db_train_f1:.4f}\")\n",
    "        print(f\"DistilBERT Classifier - Validation F1 Score: {db_val_f1:.4f}\")\n",
    "        \n",
    "        # Run comprehensive evaluation\n",
    "        db_y_pred, db_y_true, db_y_pred_proba = evaluate_model(\"DistilBERT Classifier\", db_val_preds, num_classes)\n",
    "        \n",
    "        model_names.append(\"DistilBERT Classifier\")\n",
    "        val_scores.append(db_val_f1)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error with DistilBERT Classifier: {e}\")\n",
    "        print(\"Skipping DistilBERT Classifier\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec488efb-7d44-4b5c-8c3b-35d7cc174d68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No models were run, skipping test set evaluation.\n\n=== Transformer Models Pipeline Complete ===\n"
     ]
    }
   ],
   "source": [
    "# ==================== MODEL COMPARISON AND BEST MODEL SELECTION ====================\n",
    "if model_names:\n",
    "    # Find the best model based on validation F1 scores\n",
    "    best_model_index = val_scores.index(max(val_scores))\n",
    "    best_model_name = model_names[best_model_index]\n",
    "    print(f\"\\nBest Model: {best_model_name} with Validation F1: {max(val_scores):.4f}\")\n",
    "    \n",
    "    # Get the corresponding best model for test evaluation\n",
    "    if best_model_name == \"Transformer Classifier\" and 'tc_best_model' in locals():\n",
    "        best_model = tc_best_model\n",
    "        test_data_processed = feature_model.transform(test_data)\n",
    "        test_data_final = test_data_processed.withColumn(\"embedding_array\", extract_udf(\"sentence_embeddings\")) \\\n",
    "                                           .withColumn(\"transformer_features\", array_to_vector(\"embedding_array\"))\n",
    "    elif best_model_name == \"BERT + MLP\" and 'bert_best_model' in locals():\n",
    "        best_model = bert_best_model\n",
    "        test_data_processed = feature_model.transform(test_data)\n",
    "        test_data_final = test_data_processed.withColumn(\"embedding_array\", extract_udf(\"sentence_embeddings\")) \\\n",
    "                                           .withColumn(\"transformer_features\", array_to_vector(\"embedding_array\"))\n",
    "    elif best_model_name == \"Transformer-Inspired MLP\" and 'tmlp_best_model' in locals():\n",
    "        best_model = tmlp_best_model\n",
    "        test_attention, _ = create_attention_features(test_data, feature_cols)\n",
    "        test_data_final = transformer_assembler.transform(test_attention)\n",
    "    elif best_model_name == \"DistilBERT Classifier\" and 'db_best_model' in locals():\n",
    "        best_model = db_best_model\n",
    "        test_data_processed = distilbert_model.transform(test_data)\n",
    "        test_data_final = test_data_processed.withColumn(\"embedding_array\", extract_udf(\"sentence_embeddings\")) \\\n",
    "                                           .withColumn(\"distilbert_features\", array_to_vector(\"embedding_array\"))\n",
    "    \n",
    "    if 'best_model' in locals() and 'test_data_final' in locals():\n",
    "        # Make predictions on the test set\n",
    "        test_predictions = best_model.transform(test_data_final)\n",
    "        \n",
    "        # Initialize the evaluator for F1 score\n",
    "        evaluator = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='f1')\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        test_f1 = evaluator.evaluate(test_predictions)\n",
    "        print(f\"Test F1 Score with {best_model_name}: {test_f1:.4f}\")\n",
    "        \n",
    "        # Run comprehensive evaluation on test set\n",
    "        print(f\"\\n--- {best_model_name} Test Set Evaluation ---\")\n",
    "        test_y_pred, test_y_true, test_y_pred_proba = evaluate_model(\n",
    "            f\"{best_model_name} (Test)\", test_predictions, num_classes,\n",
    "            has_probability=('Classifier' in best_model_name)\n",
    "        )\n",
    "    else:\n",
    "        print(\"Best model not available for test evaluation\")\n",
    "        \n",
    "else:\n",
    "    print(\"No models were run, skipping test set evaluation.\")\n",
    "\n",
    "print(\"\\n=== Transformer Models Pipeline Complete ===\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Transformers models",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
