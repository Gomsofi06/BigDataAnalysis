{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96816ed7-b08a-4ca3-abb9-f99880c3535d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Overview\n",
    "\n",
    "This notebook will show you how to create and query a table or DataFrame that you uploaded to DBFS. [DBFS](https://docs.databricks.com/user-guide/dbfs-databricks-file-system.html) is a Databricks File System that allows you to store data for querying inside of Databricks. This notebook assumes that you have a file already inside of DBFS that you would like to read from.\n",
    "\n",
    "This notebook is written in **Python** so the default cell type is Python. However, you can use different languages by using the `%LANGUAGE` syntax. Python, Scala, SQL, and R are all supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aba2ee02-3fd7-4889-9eff-19b32c62d37c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from math import ceil\n",
    "\n",
    "# PySpark imports\n",
    "from pyspark.sql import SparkSession  \n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, row_number, when, lit, count, lag, expr\n",
    "\n",
    "# Import necessary libraries\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Import necessary libraries\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6482be4c-f067-47c9-b0ac-35c938b94601",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# File location and type\n",
    "file_location_train = \"dbfs:/FileStore/tables/train_df-2.csv\"\n",
    "file_location_val = \"dbfs:/FileStore/tables/val_df.csv\"\n",
    "file_location_test = \"dbfs:/FileStore/tables/test_df-2.csv\"\n",
    "\n",
    "train_data = spark.read.csv(file_location_train, header=True, inferSchema=True)\n",
    "val_data = spark.read.csv(file_location_val, header=True, inferSchema=True)\n",
    "test_data = spark.read.csv(file_location_test, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "968120c3-deee-4c24-86c7-56c2d0b80452",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n|            features|label|\n+--------------------+-----+\n|[-1.611721611721E...|    0|\n|[1.95360195360194...|    0|\n|[-9.7680097680097...|    0|\n|[-4.1025641025641...|    0|\n|[4.49328449328449...|    0|\n+--------------------+-----+\nonly showing top 5 rows\n\n+--------------------+-----+\n|            features|label|\n+--------------------+-----+\n|[-1.404639804639E...|    0|\n|[-1.408547008547E...|    0|\n|[-9.4749694749694...|    0|\n|[-7.2869352869352...|    0|\n|[-5.5677655677655...|    0|\n+--------------------+-----+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Select feature columns (all except 'label', 'time', and 'file')\n",
    "feature_cols = [col for col in train_data.columns if col not in ['label', 'time', 'file']]\n",
    "\n",
    "# Assemble features into a single vector column\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "train_data = assembler.transform(train_data).select(\"features\", \"label\")\n",
    "val_data = assembler.transform(val_data).select(\"features\", \"label\")\n",
    "\n",
    "# Display the transformed train_data to verify\n",
    "train_data.show(5)\n",
    "val_data.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "189898ae-5c37-4fb1-9369-959f356e8d62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "LOGISTIC REGRESSION:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d487f882-d4d2-4a84-bb9d-64e7241acee1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Random Search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbb56de2-d7e4-4aa7-a2e8-3eebbb9659eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Tuned Parameters:\n  regParam: 0.1\n  elasticNetParam: 0.0\nBest Coefficient Matrix:\nDenseMatrix([[  1.40450407,   2.53122411,   0.14011339,   1.97657379,\n                2.23938477,  -1.31477332,  -6.1823494 ,   2.68776755,\n                2.07758891,  -0.54064081,  -2.19668173,  -3.95818659,\n                2.29067844,  -0.66173142,  -3.38937654,  -4.6374889 ,\n                6.36666242,   5.59839772,  -0.14467872,  -1.89006418,\n                3.36391549,  -0.94161118,  -3.37570875],\n             [ -3.20014889,  -1.43777489,   1.07288034,  -0.44998933,\n               -1.49782714,  -1.69682513,  -5.29850688,   1.26934949,\n               -3.56387198,  -7.6315742 ,   0.94984046,   2.81197186,\n               -4.98055581,  -1.70976195,   3.06201981,   2.79480495,\n               -5.84092199, -13.01200814,  -1.071159  ,   1.39017911,\n               -5.10802035,   2.1044435 ,   3.05682764],\n             [  1.79564482,  -1.09344921,  -1.21299374,  -1.52658446,\n               -0.74155763,   3.01159845,  11.48085627,  -3.95711704,\n                1.48628307,   8.17221501,   1.24684127,   1.14621473,\n                2.68987738,   2.37149336,   0.32735673,   1.84268395,\n               -0.52574043,   7.41361042,   1.21583772,   0.49988506,\n                1.74410486,  -1.16283233,   0.31888111]])\nBest Intercept Vector:\n[2.2813439947951615,-1.3906381822971023,-0.890705812498059]\nBest F1 Score: 0.8552674756135537\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"Logistic Regression Spark\").getOrCreate()\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "log_reg = LogisticRegression(labelCol='label', featuresCol='features', maxIter=5, predictionCol='prediction')\n",
    "\n",
    "# Define the parameter grid for logistic regression\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(log_reg.regParam, [0.1, 1, 10]) \\\n",
    "    .addGrid(log_reg.elasticNetParam, [0.0, 0.5]) \\\n",
    "    .build()\n",
    "\n",
    "# Initialize the evaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='f1')\n",
    "\n",
    "# Initialize CrossValidator for hyperparameter tuning\n",
    "crossval = CrossValidator(\n",
    "    estimator=log_reg,\n",
    "    estimatorParamMaps=param_grid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=5,  # Reduced to 3-fold cross-validation for faster training\n",
    "    parallelism=2  # Reduced parallelism to optimize resource usage\n",
    ")\n",
    "\n",
    "# Fit the cross-validator to the training data\n",
    "cv_model = crossval.fit(train_data)\n",
    "\n",
    "# Extract the best model\n",
    "best_model = cv_model.bestModel\n",
    "\n",
    "# Print only tuned parameters (exclude default values)\n",
    "tuned_params = ['regParam', 'elasticNetParam']\n",
    "best_params = {}\n",
    "\n",
    "for param in tuned_params:\n",
    "    if best_model.hasParam(param) and best_model.isSet(getattr(log_reg, param)):\n",
    "        best_params[param] = best_model.getOrDefault(getattr(log_reg, param))\n",
    "\n",
    "print(\"Best Tuned Parameters:\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "# Print the best parameters\n",
    "print(\"Best Coefficient Matrix:\")\n",
    "print(best_model.coefficientMatrix)\n",
    "print(\"Best Intercept Vector:\")\n",
    "print(best_model.interceptVector)\n",
    "\n",
    "# Print the best F1 score\n",
    "print(\"Best F1 Score:\", evaluator.evaluate(best_model.transform(val_data)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6960b42f-5a64-42f9-b3aa-ca80842012ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "for family choose between multinomial vs binomial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8330a6bc-00de-4319-8c80-e30d6146248f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e56bdfe-0c12-4f7a-839c-ba64c60a93a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training F1 Score: 0.906385588086969\nValidation F1 Score: 0.8554010609489878\nModel metrics for training and validation:\nTraining F1 Score: 0.906385588086969\nValidation F1 Score: 0.8554010609489878\n"
     ]
    }
   ],
   "source": [
    "# Create the logistic regression model\n",
    "log_reg = LogisticRegression(labelCol='label', featuresCol='features', maxIter=5, family='multinomial', regParam=5, elasticNetParam=0)\n",
    "\n",
    "# Fit the model to the training set\n",
    "lr_model = log_reg.fit(train_data)\n",
    "\n",
    "# Make predictions on the training and validation sets\n",
    "train_predictions = lr_model.transform(train_data)\n",
    "val_predictions = lr_model.transform(val_data)\n",
    "\n",
    "# Initialize the evaluator for F1 score\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='f1')\n",
    "\n",
    "# Calculate F1 scores\n",
    "score_train = evaluator.evaluate(train_predictions)\n",
    "score_val = evaluator.evaluate(val_predictions)\n",
    "\n",
    "print(\"Training F1 Score:\", score_train)\n",
    "print(\"Validation F1 Score:\", score_val)\n",
    "\n",
    "# Extract predictions as an array\n",
    "lr_y_pred = train_predictions.select(\"prediction\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# Display metrics\n",
    "print(\"Model metrics for training and validation:\")\n",
    "print(\"Training F1 Score:\", score_train)\n",
    "print(\"Validation F1 Score:\", score_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09c49c5d-1f1c-4375-a94d-7cb8fdcfe01b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Model overfitted with random search parameters, so let's increase the regularization parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b7903e1-503a-401c-b412-8587668206d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training F1 Score: 0.906385588086969\nValidation F1 Score: 0.8579304236958261\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import OneVsRest\n",
    "\n",
    "# Create the logistic regression model\n",
    "log_reg = LogisticRegression(labelCol='label', featuresCol='features', maxIter=5, regParam=5, elasticNetParam=0.5)\n",
    "\n",
    "# One-vs-Rest strategy\n",
    "ovr = OneVsRest(classifier=log_reg)\n",
    "\n",
    "# Fit the model to the training set\n",
    "lr_model = ovr.fit(train_data)\n",
    "\n",
    "# Make predictions on the training and validation sets\n",
    "train_predictions = lr_model.transform(train_data)\n",
    "test_predictions = lr_model.transform(val_data)\n",
    "\n",
    "# Calculate F1 scores\n",
    "score_train = evaluator.evaluate(train_predictions)\n",
    "score_test = evaluator.evaluate(test_predictions)\n",
    "\n",
    "print(\"Training F1 Score:\", score_train)\n",
    "print(\"Validation F1 Score:\", score_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e9ad9ae-d68f-4af6-84ec-a6ae3cbbd43a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Training F1 Score: 0.906385588086969\n",
    "Validation F1 Score: 0.8579304236958261"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Models",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}